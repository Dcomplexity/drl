{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504b1ed0",
   "metadata": {},
   "source": [
    "# Actor Critic Policy Optimization with TensorFlow\n",
    "\n",
    "In this notebook, we will look at policy optimization using TensorFlow. Let us quickly go through the derivation of what is policy Optimization and then we will apply this to CartPole environment - just like what we did in Chapter 6. \n",
    "\n",
    "\n",
    "### Derivation of Policy Gradient\n",
    "In Policy Optimization, we will have a neural network which takes in state `s` as input and produces the `logits` for action probabilities. \n",
    "\n",
    "The policy is parameterized by $\\theta$\n",
    "$$\\pi_\\theta(a|s)$$\n",
    "\n",
    "The agent follows the policy and generates the trajectory $\\large \\tau$ \n",
    "\n",
    "$$ s_1 \\rightarrow a_1 \\rightarrow s_2 \\rightarrow a_2 \\rightarrow .... \\rightarrow s_{T-1} \\rightarrow a_{T-1} \\rightarrow s_T \\rightarrow a_T$$ \n",
    "\n",
    "here $s_T$ is not necessarily the terminal state but some time horizon T upto which we are looking at the trajectory. \n",
    "\n",
    "The probability of trajectory $\\large \\tau$ depends on the transition probabilities $p(s_t+1 | s_t, a_t)$ and the policy $\\pi_\\theta(a_t|s_t)$. It is given by the expression:\n",
    "\n",
    "$$p_\\theta(\\tau) = p_\\theta(s_1, a_1, s_2, a_2, ..., s_T, a_T) = p(s_1)\\prod_{t=1}^{T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$$\n",
    "\n",
    "The expected return from following the policy $\\pi$ is given by:\n",
    "\n",
    "$$J(\\theta) = {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum_{t} \\gamma^t r(s_t, a_t) \\right]$$\n",
    "\n",
    "We want to find the $\\theta$ which maximizes the expected reward/return $J(\\theta)$. In other words, the optimal $\\theta=\\theta^*$ is given by expression\n",
    "\n",
    "$$\\theta^* = arg \\underset{\\theta}{max}{\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum_{t} \\gamma^t r(s_t, a_t) \\right] $$\n",
    "\n",
    "\n",
    "Moving on, let us try to find the optimal $\\theta$. To keep the notations easier to understand, we will replace $\\sum_{t} \\gamma^t r(s_t, a_t)$ as $r(\\tau)$:\n",
    "\n",
    "$$J(\\theta) = {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ r(\\tau) \\right] = \\int p_\\theta(\\tau)r(\\tau) d\\tau$$\n",
    "\n",
    "We take the gradient/derivative of above expression with respect to $\\theta$:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\nabla_{\\theta} \\int p_\\theta(\\tau)r(\\tau) d\\tau $$\n",
    "\n",
    "By linearity we can move the gradient inside the integral:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\int \\nabla_{\\theta} p_\\theta(\\tau)r(\\tau) d\\tau $$\n",
    "\n",
    "Using log derivative trick, we know that $\\nabla_x f(x) = f(x) \\nabla_x \\log{f(x)}$. Using this we can write above expression as:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\int p_\\theta(\\tau) \\left[ \\nabla_{\\theta}\\log{p_\\theta(\\tau)} r(\\tau) \\right] d\\tau $$\n",
    "\n",
    "We can now write the integral back as expectation, which gives us the expression:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\nabla_{\\theta}\\log{p_\\theta(\\tau)} r(\\tau) \\right] $$\n",
    "\n",
    "Let us now expand the term $\\nabla_{\\theta}\\log{p_\\theta(\\tau)}$ by writing out the full expression of $p_\\theta(\\tau)$. \n",
    "\n",
    "$$ \\nabla_{\\theta}\\log{p_\\theta(\\tau)}  = \\nabla_{\\theta} \\log{ \\left[ p(s_1) \\prod_{t=1}^{T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\\right]}$$\n",
    "\n",
    "We know that log of product of terms can be written as sum of log of terms i.e. \n",
    "\n",
    "$$\\log{\\prod_i f_i(x)} = \\sum_i log{f_i(x)}$$ \n",
    "\n",
    "Using the above substitution, we get:\n",
    "\n",
    "$$ \\nabla_{\\theta}\\log{p_\\theta(\\tau)}  = \\nabla_{\\theta} \\left[ log{p(s_1)} +  \\sum_{t=1}^{T} \\left\\{ \\log{ \\pi_\\theta(a_t|s_t)} + \\log{p(s_{t+1}|s_t,a_t)} \\right\\} \\right]$$\n",
    "\n",
    "The only term dependent on $\\theta$ is $\\pi_\\theta(a_t|s_t)$. The other two terms $log{p(s_1)}$ and $\\log{p(s_{t+1}|s_t,a_t)}$ do not depend on $\\theta$. Accordingly, we can simplify the above expression as:\n",
    "\n",
    "$$ \\nabla_{\\theta}\\log{p_\\theta(\\tau)}  = \\sum_{t=1}^{T} \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t|s_t)} $$\n",
    "\n",
    "\n",
    "Substituting the above term into the expression for $\\nabla_{\\theta} J(\\theta)$, as well as expanding $r(\\tau)$ we get:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t|s_t)} \\right) \\left( \\sum_{t=1}^{T} \\gamma^t r(s_t, a_t) \\right) \\right] $$\n",
    "\n",
    "We can now replace the outer expectation with an estimate over multiple trajectories to get the following expression for the gradient of policy objective:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\right) \\left( \\sum_{t=1}^{T} \\gamma^t r(s_t^i, a_t^i) \\right) \\right] $$\n",
    "\n",
    "where i denotes the $i^{th}$ trajectory. \n",
    "\n",
    "To improve the policy, we now take a +ve step in $\\theta$ in the direction of $\\nabla_{\\theta} J(\\theta)$:\n",
    "\n",
    "$$\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)$$\n",
    "\n",
    "To summarize, we design a model which takes state $s$ as input and produces the policy distribution $\\pi_\\theta(a|s)$ as the output of the model. We use a policy to generate returns and then change the model parameter $\\theta$ using the expression: $\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "\n",
    "### Rewards to Go Trick\n",
    "\n",
    "\n",
    "we drop the reward terms that came before time t as at time t, the action we take can only impact the reward which comes at time t and later. This leads to changing the 2nd inner sum going from t’=t to T instead of earlier sum over t’ going from t’=1 to T. i.e. the start index is now t’=t and not t=1. The revised expression is given below:\n",
    "\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\frac{1}{N} \\sum_{i=1}^{N} \\left[  \\sum_{t=1}^{T}  \\left( \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\sum_{t'=t}^{T} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i) \\right) \\right] $$\n",
    "\n",
    "\n",
    "### Baseline Trick\n",
    "We can further reduce the variance using base line to get:\n",
    "\n",
    "$$\\nabla_\\theta J\\left(\\theta\\right)=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}{\\nabla_\\theta\\log{\\pi_\\theta\\left(a_t^i\\middle| s_t^i\\right)\\ }\\left[\\hat{Q}(s_t^i,\\ a_t^i) - b\\left(s_t^i\\right)\\right]\\\\}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\hat{Q}(s_t^i,\\ a_t^i)=\\ \\sum_{t^\\prime=t}^{T} \\gamma^{t'-t} r\\left(s_{t^\\prime}^i,a_{t^\\prime}^i\\right)$$\n",
    "\n",
    "\n",
    "### Actor Critic\n",
    "In Actor Critic, we fit the baseline to an estimator for state value V. We use a model as given below:\n",
    "\n",
    "\n",
    "![Actor Critic](./images/actor_critic.png \"Actor Critic\")\n",
    "\n",
    "\n",
    "\n",
    "The final update rule under Actor Critic is given below:\n",
    "\n",
    "$$\\nabla_{\\theta,\\phi} J\\left(\\theta,\\phi \\right)=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T}{\\nabla_\\theta\\log{\\pi_\\theta\\left(a_t^i\\middle| s_t^i\\right)\\ }\\left[ \\hat{Q}(s_t^i,\\ a_t^i) -\\ V_\\phi(s_t^i)\\right]\\ }$$\n",
    "\n",
    "\n",
    "### Implementing Loss and Gradient Step in PyTorch/TensorFlow\n",
    "\n",
    "We will implement a pseudo loss function, whose derivative will give us $\\nabla_{\\theta,\\phi} J\\left(\\theta,\\phi \\right)$. Also as PyTorch/TensorFlow carryout a gradient Step, we will convert maximization to minimization by changing the sign of this objective function\n",
    "\n",
    "$$L_{CrossEntropy}(\\theta, \\phi) = - J(\\theta, \\phi) = - \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=1}^{T} \\left( \\log{\\pi_\\theta (a_t^i | s_t^i) } \\left[ \\hat{Q}(s_t^i,\\ a_t^i) - V_\\phi(s_t^i)\\right] \\right)$$\n",
    "\n",
    "To summarize, we will pass the state `s` through the network to get $\\log{ \\pi_\\theta(a_t^i|s_t^i)}$ and $V_\\phi(s_t)$. We will calculate the cross_entropy loss for the actions actually seen in the trajectory. We will then calculate the weighted mean of these individual loss terms in the trajectory with weights being the Advantage $\\hat{A}(s_t^i,\\ a_t^i) = \\hat{Q}(s_t^i,\\ a_t^i) - V_\\phi(s_t^i)$\n",
    "\n",
    "This will be followed by a gradient step in -ve direction of weighted NLL (negative log loss) i.e. in positive direction of the gradient of $J(\\theta, \\phi)= - L_{CrossEntropy}(\\theta, \\phi)$ \n",
    "\n",
    "We also add a regularization term known as Entropy. Entropy of a distribution is defined as:\n",
    "\n",
    "$$H(X) = \\sum_x -p(x).log(p(x))$$\n",
    "\n",
    "To keep enough exploration, we will want the probability to have a spread out distribution and not let the probability distribution to collapse to a single value or a small region too soon. Bigger the spread of a distribution, higher the entropy H(x) of a distribution. Accordingly, the term fed into PyTorch/TensorFlow minimizer is:\n",
    "\n",
    "\n",
    "$$Loss(\\theta, \\phi) = - J(\\theta, \\phi) - H(\\pi_\\theta(a_t^i|s_t^i)) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\sum_{t=1}^{T} \\left(  \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\left[ \\hat{Q}(s_t^i,\\ a_t^i) - V_\\phi(s_t^i)\\right] \\right) - \\beta \\sum_a \\pi_\\theta(a|s_t^i).\\log{ \\pi_\\theta(a|s_t^i)} \\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c1458b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Model, Input\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve, gaussian\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c76d46",
   "metadata": {},
   "source": [
    "### Environment - CartPole \n",
    "\n",
    "We can use the setup here to run on any environment which has state as a single vector and actions are discrete. We will build it on Cart Pole and they try to run this on many other environments like Atari games and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660c0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc2f5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "\n",
    "env = make_env(env_name)\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n\n",
    "state_dim = state_shape[0]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ca669",
   "metadata": {},
   "source": [
    "### Build Actor Critic Network\n",
    "\n",
    "We will build two simple networks that take in state. One network produces logits for the action probabilities. 2nd network produces the Value of the state. The observation space and action space is as given below for CartPole\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        \n",
    "\n",
    "Each model will be a simple one with 1 hidden layer with Relu activation and final layer being logits (for policy/actor network) and value of the state for the Critic Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9914ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.common = layers.Dense(192, activation=\"relu\")\n",
    "        self.actor = layers.Dense(n_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "model = ActorCritic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbb07b",
   "metadata": {},
   "source": [
    "### Predict Action Probabilities\n",
    "\n",
    "We will use this function to generate the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a1a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(state):\n",
    "    \"\"\"\n",
    "    params: states: [batch, state_dim]\n",
    "    returns: probs: [batch, n_actions]\n",
    "    \"\"\"\n",
    "    logits,_ = model(state)\n",
    "    action_probs = tf.nn.softmax(logits, -1).numpy()[0]\n",
    "    action = np.random.choice(n_actions, p=action_probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4adbd5",
   "metadata": {},
   "source": [
    "### Play game and generate Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eefa2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Play a session and genrate a trajectory\n",
    "    returns: arrays of states, actions, rewards\n",
    "    \"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    # initialize the environment\n",
    "    s = env.reset()\n",
    "    \n",
    "    #generate n_steps of trajectory:\n",
    "    for t in range(n_steps):\n",
    "        #sample action based on action_probs\n",
    "        a = sample_action(np.array([s], dtype=np.float32))\n",
    "        next_state, r, done, _ = env.step(a)\n",
    "        \n",
    "        #update arrays\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926b33d",
   "metadata": {},
   "source": [
    "### Calculate Rewards to Go\n",
    "\n",
    " $G(s_t) = \\sum_{t'=t}^{T} \\gamma^{t-t'} r(s_{t'}^i, a_{t'}^i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829c72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards_to_go(rewards, gamma=0.99):\n",
    "    \n",
    "    T = len(rewards) # total number of individual rewards\n",
    "    # empty array to return the rewards to go\n",
    "    rewards_to_go = [0]*T \n",
    "    rewards_to_go[T-1] = rewards[T-1]\n",
    "    \n",
    "    for i in range(T-2, -1, -1): #go from T-2 to 0\n",
    "        rewards_to_go[i] = gamma * rewards_to_go[i+1] + rewards[i]\n",
    "    \n",
    "    return rewards_to_go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296bb3b",
   "metadata": {},
   "source": [
    "### Train on one trajectory\n",
    "\n",
    "We will calculate the loss and take a gradient step. We will use Adam Optimizer\n",
    "\n",
    "**policy network loss:**\n",
    "\n",
    "We are taking only one trajectory. so N=1. We will, however, average it over the number of actions to get the average loss. So the function we will actually implement is as given below:\n",
    "\n",
    "\n",
    "$$Loss(\\theta, \\phi) = - J(\\theta, \\phi) - H(\\pi_\\theta(a_t|s_t)) = - \\frac{1}{T}  \\left[ \\sum_{t=1}^{T} \\left(  \\log{ \\pi_\\theta(a_t|s_t)} \\left[ \\hat{Q}(s_t,\\ a_t) - V_\\phi(s_t)\\right] \\right) - \\beta \\sum_{a} \\pi_\\theta(a|s_t).\\log{ \\pi_\\theta(a|s_t)} \\right] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f52c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def train_one_episode(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    # get rewards to go\n",
    "    rewards_to_go = get_rewards_to_go(rewards, gamma)\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int)\n",
    "    rewards_to_go = np.array(rewards_to_go, dtype=np.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # get action probabilities from states\n",
    "        logits, state_values = model(states)\n",
    "        probs = tf.nn.softmax(logits, -1)\n",
    "        log_probs = tf.nn.log_softmax(logits, -1)\n",
    "        \n",
    "        row_indices = tf.range(len(actions))\n",
    "        indices = tf.transpose([row_indices, actions])\n",
    "        log_probs_for_actions = tf.gather_nd(log_probs, indices)\n",
    "        \n",
    "        advantage = rewards_to_go - state_values\n",
    "        \n",
    "        # Compute loss to be minized\n",
    "        J = tf.reduce_mean(log_probs_for_actions * advantage)\n",
    "        H = -tf.reduce_mean(tf.reduce_sum(probs * log_probs, -1))\n",
    "        \n",
    "        loss = -(J + entropy_coef * H)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return np.sum(rewards) # to show progress on training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f78dd",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182cec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-912679cafead>:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  actions = np.array(actions, dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:28.576\n",
      "mean reward:52.000\n",
      "mean reward:121.545\n",
      "mean reward:239.556\n",
      "mean reward:299.525\n",
      "mean reward:850.283\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "for i in range(10000):\n",
    "    states, actions, rewards = generate_trajectory(env)\n",
    "    reward = train_one_episode(states, actions, rewards)\n",
    "    total_rewards.append(reward)\n",
    "    if i != 0 and i % 100 == 0:\n",
    "        mean_reward = np.mean(total_rewards[-100:-1])\n",
    "        print(\"mean reward:%.3f\" % (mean_reward))\n",
    "        if mean_reward > 700:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7a42e",
   "metadata": {},
   "source": [
    "### Let us record a video of trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5618b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(env, save_dir):\n",
    "    try:\n",
    "        env = gym.wrappers.Monitor(\n",
    "            env, save_dir, video_callable=lambda id: True, force=True, mode='evaluation')\n",
    "    except gym.error.Error as e:\n",
    "        print(e)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    generate_trajectory(env)\n",
    "    \n",
    "def display_animation(filepath):\n",
    "    video = io.open(filepath, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b106640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAAhtZGF0AAAA1m1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAAAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4LjQ1LjEwMA==\" type=\"video/mp4\" />\n",
       "                 </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Animate learned policy\n",
    "save_dir='./videos/tensorflow/actor_critic/'\n",
    "env = make_env(env_name)\n",
    "generate_animation(env, save_dir=save_dir)\n",
    "[filepath] = glob.glob(os.path.join(save_dir, '*.mp4'))\n",
    "display_animation(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127f460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
