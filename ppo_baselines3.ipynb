{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b00335d",
   "metadata": {},
   "source": [
    "# PPO agent using stable-baselines3\n",
    "\n",
    "In this notebook, we will look at policy optimization using PPO. We will not be writing our own algorithms. Rather, we will use a fork of [OpenAI Baselines](https://github.com/openai/baselines). The version we will use is still in beta but good enough for your tutorials. THe github repository for baselines-3 : https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "\n",
    "The code below follows the [getting started tutorial](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb) that comes with this library.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb76bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bb16c",
   "metadata": {},
   "source": [
    "### Environment - CartPole \n",
    "\n",
    "We can use the setup here to run on any environment which has state as a single vector and actions are discrete. We will build it on Cart Pole and they try to run this on many other environments like Atari games and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef8c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name)\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876b050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdeng\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 无法在设置线程模式后对其加以更改。\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "\n",
    "env = make_env(env_name)\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n\n",
    "state_dim = state_shape[0]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e6c15",
   "metadata": {},
   "source": [
    "### Build Actor Critic Network\n",
    "\n",
    "We will build two simple networks that take in state. One network produces logits for the action probabilities. 2nd network produces the Value of the state. The observation space and action space is as given below for CartPole\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        \n",
    "\n",
    "Each model will be a simple one with 1 hidden layer with Relu activation and final layer being logits (for policy/actor network) and value of the state for the Critic Network.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86f4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec5720",
   "metadata": {},
   "source": [
    "### Untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1af6b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdeng\\anaconda3\\envs\\pytorch\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:8.99 +/- 0.62\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681480d0",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a8d604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x294f4e7ff70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 30000 steps\n",
    "model.learn(total_timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b30eae",
   "metadata": {},
   "source": [
    "### Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521f95fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:  500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward: {mean_reward: .2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4af04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
