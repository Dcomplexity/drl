{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mechanical-discovery",
   "metadata": {},
   "source": [
    "### Q Learning Update equation (remains same)\n",
    "\n",
    "Q Learning control is carried by sampling step by step and updating Q values at each step. We use ε-greedy policy to explore and generate samples. However, the policy learnt is a deterministic greedy policy with no exploration. The Update equation is given below:\n",
    "\n",
    "$$ \n",
    "\\DeclareMathOperator*{\\max}{max} Q(S,A) \\leftarrow Q(S,A) + \\alpha * [ R + \\gamma * \\max_{A'} Q(S’,A’) – Q(S,A)] $$\n",
    "\n",
    "\n",
    "In experience replay, we store the samples `(s, a, r, s', done)` in a buffer. The samples are generated using an exploratory behavior policy while we improve a deterministic target policy using Q-values. Therefore, we can always use older samples from behavior policy and apply them again and again. We can keep the buffer size fixed to some pre-determined size and keep deleting the older samples as we collect new ones. The process makes learning efficient by reusing a sample multiple times. Rest of the approach remains same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-crazy",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closing-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and enviroment setup\n",
    "import sys\n",
    "import gym\n",
    "import gym.envs.toy_text\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "particular-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Learning agent class\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, gamma, get_possible_actions):\n",
    "        self.get_possible_actions = get_possible_actions\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self._Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    def get_Q(self, state, action):\n",
    "        return self._Q[state][action]\n",
    "\n",
    "    def set_Q(self, state, action, value):\n",
    "        self._Q[state][action] = value\n",
    "\n",
    "    # Q learning update step\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if not done:\n",
    "            best_next_action = self.max_action(next_state)\n",
    "            td_error = reward + self.gamma * \\\n",
    "                self.get_Q(next_state, best_next_action) - \\\n",
    "                self.get_Q(state, action)\n",
    "        else:\n",
    "            td_error = reward - self.get_Q(state, action)\n",
    "\n",
    "        new_value = self.get_Q(state, action) + self.alpha * td_error\n",
    "        self.set_Q(state, action, new_value)\n",
    "\n",
    "    # get best A for Q(S,A) which maximizes the Q(S,a) for actions in state S\n",
    "    def max_action(self, state):\n",
    "        actions = self.get_possible_actions(state)\n",
    "        best_action = []\n",
    "        best_q_value = float(\"-inf\")\n",
    "\n",
    "        for action in actions:\n",
    "            q_s_a = self.get_Q(state, action)\n",
    "            if q_s_a > best_q_value:\n",
    "                best_action = [action]\n",
    "                best_q_value = q_s_a\n",
    "            elif q_s_a == best_q_value:\n",
    "                best_action.append(action)\n",
    "        return np.random.choice(np.array(best_action))\n",
    "\n",
    "    # choose action as per epsilon-greedy policy for exploration\n",
    "    def get_action(self, state):\n",
    "        actions = self.get_possible_actions(state)\n",
    "\n",
    "        if len(actions) == 0:\n",
    "            return None\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            a = np.random.choice(actions)\n",
    "            return a\n",
    "        else:\n",
    "            a = self.max_action(state)\n",
    "            return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welcome-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "def plot_rewards(env_name, rewards, label):\n",
    "    plt.title(\"env={}, Mean reward = {:.1f}\".format(env_name,\n",
    "                                                    np.mean(rewards[-20:])))\n",
    "    plt.plot(rewards, label=label)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.ylim(-300, 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "linear-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print policy Cliff world\n",
    "def print_policy(env, agent):\n",
    "    nR, nC = env._cliff.shape\n",
    "\n",
    "    actions = '^>v<'\n",
    "\n",
    "    for y in range(nR):\n",
    "        for x in range(nC):\n",
    "            if env._cliff[y, x]:\n",
    "                print(\" C \", end='')\n",
    "            elif (y * nC + x) == env.start_state_index:\n",
    "                print(\" X \", end='')\n",
    "            elif (y * nC + x) == nR * nC - 1:\n",
    "                print(\" T \", end='')\n",
    "            else:\n",
    "                print(\" %s \" %\n",
    "                      actions[agent.max_action(y * nC + x)], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-clothing",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amateur-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size  # max number of items in buffer\n",
    "        self.buffer = []   # array to hold buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.buffer = self.buffer[-self.size:] + [item]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return states, actions, rewards, next_states, done_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "another-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training algorithm with reply buffer\n",
    "def train_agent(env, agent, episode_cnt=10000, tmax=10000,\n",
    "                anneal_eps=True, replay_buffer=None, batch_size=16):\n",
    "    episode_rewards = []\n",
    "    for i in range(episode_cnt):\n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "        for t in range(tmax):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if replay_buffer:\n",
    "                replay_buffer.add(state, action, reward, next_state, done)\n",
    "                states, actions, rewards, next_states, done_flags = \\\n",
    "                        replay_buffer(batch_size)\n",
    "                for i in range(batch_size):\n",
    "                    agent.update(states[i], actions[i], rewards[i],\n",
    "                                 next_states[i], done_flags[i])\n",
    "            else:\n",
    "                agent.update(state, action, reward, next_state, done)\n",
    "\n",
    "            G += reward\n",
    "            if done:\n",
    "                episode_rewards.append(G)\n",
    "                # to reduce the exploration probability epsilon over the\n",
    "                # training period.\n",
    "                if anneal_eps:\n",
    "                    agent.epsilon = agent.epsilon * 0.99\n",
    "                break\n",
    "            state = next_state\n",
    "    return np.array(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "independent-sphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is a simple implementation of the Gridworld Cliff\n",
      "    reinforcement learning task.\n",
      "\n",
      "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
      "    by Sutton and Barto:\n",
      "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
      "\n",
      "    With inspiration from:\n",
      "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
      "\n",
      "    The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
      "        [3, 0] as the start at bottom-left\n",
      "        [3, 11] as the goal at bottom-right\n",
      "        [3, 1..10] as the cliff at bottom-center\n",
      "\n",
      "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
      "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# create cliff world environment\n",
    "env = gym.envs.toy_text.CliffWalkingEnv()\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "assisted-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Q Learning agent\n",
    "agent = QLearningAgent(alpha=0.25, epsilon=0.2, gamma=0.99, \n",
    "                       get_possible_actions=lambda s : range(env.nA))\n",
    "\n",
    "# train agent using replay buffer and get rewards for episodes\n",
    "rewards = train_agent(env, agent, episode_cnt=5000, \n",
    "                      replay_buffer=ReplayBuffer(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "heavy-refund",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot per episode reward\n",
    "plot_rewards(\"Cliff World\", rewards, 'Q-Learning(Replay Bffer)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alert-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ^  >  >  >  >  >  >  ^  >  >  v  v \n",
      " <  >  v  >  >  >  >  >  v  v  >  v \n",
      " >  >  >  >  >  >  >  >  >  >  >  v \n",
      " X  C  C  C  C  C  C  C  C  C  C  T \n"
     ]
    }
   ],
   "source": [
    "# print policy learnt by the agent\n",
    "print_policy(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-commission",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl_tensorflow]",
   "language": "python",
   "name": "conda-env-rl_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
