{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8282f461",
   "metadata": {},
   "source": [
    "# REINFORCE Policy Optimization with TensorFlow\n",
    "\n",
    "In this notebook, we will look at policy optimization using TensorFlow. Let us quickly go through the derivation of what is policy Optimization and then we will apply this to CartPole environment - just like what we did in Chapter 6. \n",
    "\n",
    "\n",
    "### Derivation of Policy Gradient\n",
    "In Policy Optimization, we will have a neural network which takes in state `s` as input and produces the `logits` for action probabilities. \n",
    "\n",
    "The policy is parameterized by $\\theta$\n",
    "$$\\pi_\\theta(a|s)$$\n",
    "\n",
    "The agent follows the policy and generates the trajectory $\\large \\tau$ \n",
    "\n",
    "$$ s_1 \\rightarrow a_1 \\rightarrow s_2 \\rightarrow a_2 \\rightarrow .... \\rightarrow s_{T-1} \\rightarrow a_{T-1} \\rightarrow s_T \\rightarrow a_T$$ \n",
    "\n",
    "here $s_T$ is not necessarily the terminal state but some time horizon T upto which we are looking at the trajectory. \n",
    "\n",
    "The probability of trajectory $\\large \\tau$ depends on the transition probabilities $p(s_t+1 | s_t, a_t)$ and the policy $\\pi_\\theta(a_t|s_t)$. It is given by the expression:\n",
    "\n",
    "$$p_\\theta(\\tau) = p_\\theta(s_1, a_1, s_2, a_2, ..., s_T, a_T) = p(s_1)\\prod_{t=1}^{T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$$\n",
    "\n",
    "The expected return from following the policy $\\pi$ is given by:\n",
    "\n",
    "$$J(\\theta) = {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum_{t} \\gamma^t r(s_t, a_t) \\right]$$\n",
    "\n",
    "We want to find the $\\theta$ which maximizes the expected reward/return $J(\\theta)$. In other words, the optimal $\\theta=\\theta^*$ is given by expression\n",
    "\n",
    "$$\\theta^* = arg \\underset{\\theta}{max}{\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum_{t} \\gamma^t r(s_t, a_t) \\right] $$\n",
    "\n",
    "Moving on, let us try to find the optimal $\\theta$. To keep the notations easier to understand, we will replace $\\sum_{t} \\gamma^t r(s_t, a_t)$ as $r(\\tau)$:\n",
    "\n",
    "$$J(\\theta) = {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ r(\\tau) \\right] = \\int p_\\theta(\\tau)r(\\tau) d\\tau$$\n",
    "\n",
    "We take the gradient/derivative of above expression with respect to $\\theta$:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\nabla_{\\theta} \\int p_\\theta(\\tau)r(\\tau) d\\tau $$\n",
    "\n",
    "By linearity we can move the gradient inside the integral:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\int \\nabla_{\\theta} p_\\theta(\\tau)r(\\tau) d\\tau $$\n",
    "\n",
    "Using log derivative trick, we know that $\\nabla_x f(x) = f(x) \\nabla_x \\log{f(x)}$. Using this we can write above expression as:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\int p_\\theta(\\tau) \\left[ \\nabla_{\\theta}\\log{p_\\theta(\\tau)} r(\\tau) \\right] d\\tau $$\n",
    "\n",
    "We can now write the integral back as expectation, which gives us the expression:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\nabla_{\\theta}\\log{p_\\theta(\\tau)} r(\\tau) \\right] $$\n",
    "\n",
    "Let us now expand the term $\\nabla_{\\theta}\\log{p_\\theta(\\tau)}$ by writing out the full expression of $p_\\theta(\\tau)$. \n",
    "\n",
    "$$ \\nabla_{\\theta}\\log{p_\\theta(\\tau)}  = \\nabla_{\\theta} \\log{ \\left[ p(s_1) \\prod_{t=1}^{T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)\\right]}$$\n",
    "\n",
    "We know that log of product of terms can be written as sum of log of terms i.e. \n",
    "\n",
    "$$\\log{\\prod_i f_i(x)} = \\sum_i log{f_i(x)}$$ \n",
    "\n",
    "Using the above substitution, we get:\n",
    "\n",
    "$$ \\nabla_{\\theta}\\log{p_\\theta(\\tau)}  = \\nabla_{\\theta} \\left[ log{p(s_1)} +  \\sum_{t=1}^{T} \\left\\{ \\log{ \\pi_\\theta(a_t|s_t)} + \\log{p(s_{t+1}|s_t,a_t)} \\right\\} \\right]$$\n",
    "\n",
    "The only term dependent on $\\theta$ is $\\pi_\\theta(a_t|s_t)$. The other two terms $log{p(s_1)}$ and $\\log{p(s_{t+1}|s_t,a_t)}$ do not depend on $\\theta$. Accordingly, we can simplify the above expression as:\n",
    "\n",
    "$$ \\nabla_{\\theta}\\log{p_\\theta(\\tau)}  = \\sum_{t=1}^{T} \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t|s_t)} $$\n",
    "\n",
    "\n",
    "Substituting the above term into the expression for $\\nabla_{\\theta} J(\\theta)$, as well as expanding $r(\\tau)$ we get:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  {\\large E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t|s_t)} \\right) \\left( \\sum_{t=1}^{T} \\gamma^t r(s_t, a_t) \\right) \\right] $$\n",
    "\n",
    "We can now replace the outer expectation with an estimate over multiple trajectories to get the following expression for the gradient of policy objective:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\right) \\left( \\sum_{t=1}^{T} \\gamma^t r(s_t^i, a_t^i) \\right) \\right] $$\n",
    "\n",
    "where i denotes the $i^{th}$ trajectory. \n",
    "\n",
    "To improve the policy, we now take a +ve step in $\\theta$ in the direction of $\\nabla_{\\theta} J(\\theta)$:\n",
    "\n",
    "$$\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)$$\n",
    "\n",
    "To summarize, we design a model which takes state $s$ as input and produces the policy distribution $\\pi_\\theta(a|s)$ as the output of the model. We use a policy to generate returns and then change the model parameter $\\theta$ using the expression: $\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "\n",
    "### Rewards to Go Trick\n",
    "\n",
    "\n",
    "we drop the reward terms that came before time t as at time t, the action we take can only impact the reward which comes at time t and later. This leads to changing the 2nd inner sum going from t’=t to T instead of earlier sum over t’ going from t’=1 to T. i.e. the start index is now t’=t and not t=1. The revised expression is given below:\n",
    "\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\frac{1}{N} \\sum_{i=1}^{N} \\left[  \\sum_{t=1}^{T}  \\left( \\nabla_{\\theta} \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\sum_{t'=t}^{T} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i) \\right) \\right] $$\n",
    "\n",
    "\n",
    "### Implementing Loss and Gradient Step in PyTorch/TensorFlow\n",
    "\n",
    "We will implement a pseudo loss function, whose derivative will give us $\\nabla_{\\theta} J(\\theta)$. Also as PyTorch/TensorFlow carryout a gradient Step, we will convert maximization to minimization by changing the sign of this objective function\n",
    "\n",
    "$$L_{CrossEntropy}(\\theta) = - J(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\sum_{t=1}^{T} \\left(  \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\sum_{t'=t}^{T} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i) \\right) \\right] $$\n",
    "\n",
    "To summarize, we will pass the state `s` through the network to get $\\log{ \\pi_\\theta(a_t^i|s_t^i)}$. We will calculate the cross_entropy loss for the actions actually seen in the trajectory. We will then calculate the weighted mean of these individual loss terms in the trajectory with weights being the rewards-to-go $\\sum_{t'=t}^{T} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i)$\n",
    "\n",
    "This will be followed by a gradient step in -ve direction of weighted NLL (negative log loss) i.e. in positive direction of the gradient of $J(\\theta)= - L_{CrossEntropy}(\\theta)$ \n",
    "\n",
    "We also add a regularization term known as Entropy. Entropy of a distribution is defined as:\n",
    "\n",
    "$$H(X) = \\sum_x -p(x).log(p(x))$$\n",
    "\n",
    "To keep enough exploration, we will want the probability to have a spread out distribution and not let the probability distribution to collapse to a single value or a small region too soon. BIgger the spread of a distribution, higher the entropy H(x) of a distribution. Accordingly, the term fed into PyTorch/TensorFlow minimizer is:\n",
    "\n",
    "\n",
    "$$Loss(\\theta) = - J(\\theta) - H(\\pi_\\theta(a_t^i|s_t^i)) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\sum_{t=1}^{T} \\left(  \\log{ \\pi_\\theta(a_t^i|s_t^i)} \\sum_{t'=t}^{T} \\gamma^{t'-t} r(s_{t'}^i, a_{t'}^i) \\right) - \\beta \\sum_{a_i} \\pi_\\theta(a_t^i|s_t^i).\\log{ \\pi_\\theta(a_t^i|s_t^i)} \\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e880e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve, gaussian\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcfbba",
   "metadata": {},
   "source": [
    "### Environment - CartPole \n",
    "\n",
    "We can use the setup here to run on any environment which has state as a single vector and actions are discrete. We will build it on Cart Pole and they try to run this on many other environments like Atari games and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a282bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7cab2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "\n",
    "env = make_env(env_name)\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n\n",
    "state_dim = state_shape[0]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a59dee",
   "metadata": {},
   "source": [
    "### Build Policy Network\n",
    "\n",
    "We will build a simple network that takes in state and produces logits for the action probabilities. We will keep network simple. he observation space and action space is as given below for CartPole\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        \n",
    "\n",
    "The model will be a simple one with 1 hidden layer with Relu activation and final layer being logits with dimension equal to number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257bf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(state_dim,)),\n",
    "    tf.keras.layers.Dense(n_actions)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76bda9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98f585",
   "metadata": {},
   "source": [
    "### Predict Action Probabilities\n",
    "\n",
    "We will use this function to generate the trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548cb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\"\n",
    "    params: states: [batch, state_dim]\n",
    "    returns: probs: [batch, n_actions]\n",
    "    \"\"\"\n",
    "    logits = model(states)\n",
    "    probs = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dac8e",
   "metadata": {},
   "source": [
    "### Play game and generate Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1cf7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Play a session and generate a trajectory\n",
    "    returns: arrays of states, actions, rewards\n",
    "    \"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    # initialize the environment\n",
    "    s = env.reset()\n",
    "    \n",
    "    # generate n_steps of trajectory:\n",
    "    for t in range(n_steps):\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "        # sample action based on action_probs\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        next_state, r, done, _ = env.step(a)\n",
    "        \n",
    "        # update arrays\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c9510",
   "metadata": {},
   "source": [
    "### Calculate Rewards to Go\n",
    "\n",
    " $G(s_t) = \\sum_{t'=t}^{T} \\gamma^{t-t'} r(s_{t'}^i, a_{t'}^i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153efef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards_to_go(rewards, gamma=0.99):\n",
    "    T = len(rewards) # total number of individual rewards\n",
    "    # empty array to return the rewards to go\n",
    "    rewards_to_go = [0] * T\n",
    "    rewards_to_go[T - 1] = rewards[T - 1]\n",
    "    \n",
    "    for i in range(T-2, -1, -1): # go from T-2 to 0\n",
    "        rewards_to_go[i] = gamma * rewards_to_go[i + 1] + rewards[i]\n",
    "        \n",
    "    return rewards_to_go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613e2e5",
   "metadata": {},
   "source": [
    "### Train on one trajectory\n",
    "\n",
    "We will calculate the loss and take a gradient step. We will use Adam Optimizer\n",
    "\n",
    "We are taking only one trajectory. so N=1. We will however, average it over the number of actions to get the average loss. So the function we will actually implement is as given below:\n",
    "\n",
    "$$Loss(\\theta) = - J(\\theta) - H(\\pi_\\theta(a_t|s_t)) = - \\frac{1}{T}  \\sum_{t=1}^{T} \\left( \\log{ \\pi_\\theta(a_t|s_t)} G(s_t) - \\beta \\sum_{a_i} \\pi_\\theta(a_t|s_t).\\log{ \\pi_\\theta(a_t|s_t)} \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b3835ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def train_one_episode(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    # get rewards to go\n",
    "    rewards_to_go = get_rewards_to_go(rewards, gamma)\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards_to_go = np.array(rewards_to_go)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # get actioin probabilities from states\n",
    "        logits = model(states)\n",
    "        probs = tf.nn.softmax(logits, -1)\n",
    "        log_probs = tf.nn.log_softmax(logits, -1)\n",
    "        \n",
    "        row_indices = tf.range(len(actions))\n",
    "        indices = tf.transpose([row_indices, actions])\n",
    "        log_probs_for_actions = tf.gather_nd(log_probs, indices)\n",
    "        \n",
    "        # Compute loss to be minized\n",
    "        J = tf.reduce_mean(log_probs_for_actions * rewards_to_go)\n",
    "        H = -tf.reduce_mean(tf.reduce_sum(probs * log_probs, -1))\n",
    "        \n",
    "        loss = -(J + entropy_coef * H)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return np.sum(rewards) # to show progress on training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85550b",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c30c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:31.061\n",
      "mean reward:45.939\n",
      "mean reward:65.141\n",
      "mean reward:121.606\n",
      "mean reward:163.667\n",
      "mean reward:154.909\n",
      "mean reward:237.354\n",
      "mean reward:390.374\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "for i in range(10000):\n",
    "    states, actions, rewards = generate_trajectory(env)\n",
    "    reward = train_one_episode(states, actions, rewards)\n",
    "    total_rewards.append(reward)\n",
    "    if i != 0 and i % 100 == 0:\n",
    "        mean_reward = np.mean(total_rewards[-100: -1])\n",
    "        print(\"mean reward:%.3f\" % (mean_reward))\n",
    "        if mean_reward > 300:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352be485",
   "metadata": {},
   "source": [
    "**Let us record a video of trained agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f4fd625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(env, save_dir):\n",
    "    try:\n",
    "        env = gym.wrappers.Monitor(\n",
    "            env, save_dir, video_callable=lambda id: True, force=True, mode='evaluation')\n",
    "    except gym.error.Error as e:\n",
    "        print(e)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    generate_trajectory(env)\n",
    "\n",
    "\n",
    "\n",
    "def display_animation(filepath):\n",
    "    video = io.open(filepath, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dc0e801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAAhtZGF0AAAA1m1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAAAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4LjQ1LjEwMA==\" type=\"video/mp4\" />\n",
       "                 </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Animate learned policy\n",
    "save_dir='./videos/tensorflow/reinforce/'\n",
    "env = make_env(env_name)\n",
    "generate_animation(env, save_dir=save_dir)\n",
    "[filepath] = glob.glob(os.path.join(save_dir, '*.mp4'))\n",
    "display_animation(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92dcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
